# üì¶ Non-Contiguity in PyTorch: Impact on Training Performance

This repository investigates the impact of **non-contiguous tensors** on different stages of deep learning training in PyTorch. It explores how non-contiguous memory layouts affect data loading, transformations, model computations, and training efficiency.

---

## üìå Objectives

1. **Training Pipeline Overhead**  
   Analyze how non-contiguous tensors affect the standard training pipeline using a benchmark dataset and model.

2. **Contiguity Conversion Cost**  
   Measure the overhead of converting non-contiguous tensors to contiguous form for various tensor sizes and shapes.

3. **Transform Operation Effects**  
   Understand how individual transformation operations affect tensor contiguity, execution time, and memory layout.

4. **Model Layer Contiguity Impact**  
   Evaluate training performance and memory usage when model layers operate on contiguous vs non-contiguous tensors.

---

## üóÇÔ∏è Repository Structure

| Script | Purpose |
|--------|---------|
| `cifar10Benchmark.py` | Benchmarks standard training with/without ensuring tensor contiguity during transformation. |
| `cpu_contig_benchmark.py` | Measures the cost of making tensors contiguous for various sizes and shapes on CPU. |
| `customAug_2.py` | Evaluates the effect of each transformation operation on tensor contiguity and timing. |
| `profile_cifar10_cnn.py` | Profiles CNN training performance and memory usage with three training strategies. |

---

## ‚ñ∂Ô∏è How to Run

### 1. Benchmark Training with/without Contiguity
```bash
python cifar10Benchmark.py --is_contiguous true
```
### Arguments
--is_contiguous: If true, ensures tensor is contiguous after every transformation in the pipeline.

### 2. Contiguity Cost on CPU
```bash
python cpu_contig_benchmark.py
```
Modify the list of tensor sizes in the script to explore different shape/dimension combinations.

### 3. Transformation Pipeline Analysis
```bash
python customAug_2.py
```
- Instructions:
  - Enable only one line between line 49 and 51 in the script:
  - Line 49: Applies contiguous() after each transformation (forces contiguity).
  - Line 51: Leaves tensors as-is (default PyTorch behavior).

### 4. CNN Profiling with Three Training Modes
```bash
python profile_cifar10_cnn.py

```
This script compares:
- Basic: Default forward pass.
- Manual: Converts tensor to contiguous after each layer.
- Decorated: Uses a decorated version of the forward method for profiling.

### Requirements:
- Python ‚â• 3.7
- PyTorch with CUDA support
- memory_profiler (for tracking memory usage)


## What You‚Äôll Learn
- How PyTorch manages memory for tensors during transformations and model operations.
- When and why .contiguous() is necessary in training pipelines.
- Trade-offs between forcing contiguity and relying on PyTorch‚Äôs memory layout handling.
- Performance (CPU/GPU time and memory) comparison across different tensor layouts.
